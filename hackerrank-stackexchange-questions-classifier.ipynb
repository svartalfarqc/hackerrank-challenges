{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#!/usr/bin/python\n",
    "#coding:utf-8\n",
    "\n",
    "# Configuration\n",
    "import json,sys\n",
    "\n",
    "# IMPORT\n",
    "\n",
    "# open training file data into array\n",
    "train_array=[]\n",
    "train_label=[]\n",
    "f=open('training.json')\n",
    "for i in range(int(f.readline())):\n",
    "    h=json.loads(f.readline())\n",
    "    train_array.append(h['question']+\"\\r\\n\"+h['excerpt'])\n",
    "    train_label.append(h['topic'])\n",
    "\n",
    "f.close()\n",
    "\n",
    "# open test file data into array\n",
    "test_array=[]\n",
    "for i in range(int(input())):\n",
    "    h=json.loads(input())\n",
    "    test_array.append(h['question']+\"\\r\\n\"+h['excerpt'])\n",
    "\n",
    "# Preprocess and train\n",
    "\n",
    "# HashingVectorizer vs CountVectorizer\n",
    "# from scikitlearn documentation: it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hash_vectorizer = HashingVectorizer(stop_words='english')\n",
    "\n",
    "train = hash_vectorizer.fit_transform(train_array)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "#scaler.fit(train)\n",
    "#train = scaler.transform(train)\n",
    "\n",
    "# No need to perform Term Frequency times inverse document frequency (TF-IDF)\n",
    "# Unnecessary here because it's just comments, not long documents\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# train = tfidf_transformer.fit_transform(train)\n",
    "\n",
    "# Model\n",
    "\n",
    "# First, try Naive Bayes classification\n",
    "# Doesn't work, gives negative values. MinMaxScaling generates an error\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#nb = MultinomialNB()\n",
    "#nb.fit(train,train_label)\n",
    "\n",
    "# Put test data into vectorizer\n",
    "test = hash_vectorizer.fit_transform(test_array)\n",
    "#scaler.fit(test)\n",
    "#test = scaler.transform(test)\n",
    "\n",
    "# Second, try Support Vector Machine (SVM) instead\n",
    "from sklearn.svm import LinearSVC\n",
    "svm=LinearSVC()\n",
    "svm.fit(train,train_label)\n",
    "\n",
    "\n",
    "# Predict\n",
    "#test_label = nb.predict(test)\n",
    "test_label=svm.predict(test)\n",
    "\n",
    "for e in test_label: print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
